{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf85230-b2f3-4491-ace6-6a9831a1078a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41922c8f-2123-4abb-9ffb-b1757021b4b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#access datalake --> please use private endpoint and service principal auth instead in production environment\n",
    "spark.conf.set(\"fs.azure.account.key.'your_storage_account_name'.dfs.core.windows.net\",\"'your_storage_account_key'\")\n",
    "\n",
    "display(dbutils.fs.ls(\"abfss://'your_container_name'@'your_storage_account_name'.dfs.core.windows.net\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9283cdc-4339-4d99-84d5-97a010ca1a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read the text files and create DataFrames\n",
    "df_firstcall = spark.read.text(\"abfss://'your_container_name'@'your_storage_account_name'.dfs.core.windows.net/'your_file_name'\")\n",
    "df_secondcall = spark.read.text(\"abfss://'your_container_name'@'your_storage_account_name'.dfs.core.windows.net/'your_file_name'\")\n",
    "df_thirdcall = spark.read.text(\"abfss://'your_container_name'@'your_storage_account_name'.dfs.core.windows.net/'your_file_name'\")\n",
    "\n",
    "\n",
    "raw_collector = [df_firstcall, df_secondcall, df_thirdcall]\n",
    "call_collector = []\n",
    "\n",
    "for i, df in enumerate(raw_collector):\n",
    "    # Convert the DataFrame to an RDD of strings and collect the strings\n",
    "    text_rdd = df.rdd.map(lambda x: x[0])\n",
    "    text_list = text_rdd.collect()\n",
    "\n",
    "    # Join the list of strings with newline characters to create a single text string\n",
    "    text_str = '\\n'.join(text_list)\n",
    "    call_collector.append(text_str)\n",
    "\n",
    "print(call_collector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e5a31e-bc9a-4c9a-8114-34eea0710d38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#connect with Azure Open AI service\n",
    "import os\n",
    "import requests\n",
    "import jsonb\n",
    "import openai\n",
    "\n",
    "#api call\n",
    "openai.api_key = \"'your_azureopenai_key'\"\n",
    "openai.api_base =  \"'your_azureopenai_url'\" \n",
    "openai.api_type = 'azure'\n",
    "openai.api_version = '2022-12-01' \n",
    "\n",
    "#gpt model selection | would recommend to test in playground which model suits your data at best taking into consideration price/latency/performance\n",
    "deployment_name='text-davinci-003'\n",
    "\n",
    "#summarize call trascripts\n",
    "summary_collector = []\n",
    "\n",
    "for i, call in enumerate(call_collector):\n",
    "    prompt_summary = \"summarize the following phone call:\" + call_collector[i]\n",
    "    response_summary = openai.Completion.create(engine=deployment_name, prompt=prompt_summary, max_tokens=1000, temperature=0)\n",
    "    summary = response_summary['choices'][0]['text'].replace('\\n', '').replace(' .', '.').strip()\n",
    "    summary_collector.append(summary)\n",
    "\n",
    "print (summary_collector)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#extract insights from first call (keeping column names for schema)\n",
    "csv_text_collector = \"\"\n",
    "prompt_extraction = \"From the call transcript in the next paragraph, extract the following information in csv format: customer full name, address, birth date (dd-mm-yyyy), price per month, switch reason, sentiment, starting plan date (dd-mmj-yyyy). Provide also the column names.\" + call_collector[0]\n",
    "response_info = openai.Completion.create(engine=deployment_name, prompt=prompt_extraction, max_tokens=1000, temperature=0)\n",
    "extraction = response_info['choices'][0]['text'].replace(' .', '.').strip()\n",
    "csv_text_collector += extraction\n",
    "csv_text_collector += '\\n'\n",
    "\n",
    "\n",
    "#extract insignts starting from the second call (no column names, no schema needed)\n",
    "for i in range(len(call_collector)-1):\n",
    "    prompt_extraction = \"From the call transcript in the next paragraph, extract the following information in csv format: customer full name, address, birth date (dd-mm-yyyy), price per month, switch reason, sentiment, starting plan date (dd-mmj-yyyy).\" + call_collector[i+1]\n",
    "    response_info = openai.Completion.create(engine=deployment_name, prompt=prompt_extraction, max_tokens=1000, temperature=0)\n",
    "    extraction = response_info['choices'][0]['text'].replace(' .', '.').strip()\n",
    "    csv_text_collector += extraction\n",
    "    csv_text_collector += \"\\n\"\n",
    "    \n",
    "print(csv_text_collector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0509094b-b14e-42da-80cb-727a0e1403fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#use panda to create dataframe object from string comma separated\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_text_collector))\n",
    "\n",
    "df.columns = [c.replace(' ', '_') for c in df]\n",
    "\n",
    "#renaming columns to comply with delta columns accepted format\n ",
    "df.rename(columns={\"Birth_Date_(dd-mm-yyyy)\": \"Birth_Date\", \"Starting_Plan_Date_(dd-mm-yyyy)\": \"Starting_Plan_Date\"}, inplace=True)\n",
    "\n",
    "df.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fb0c98-25ee-4e24-9a54-4f2de5813054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert panda df to spark df\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "display(spark_df)\n",
    "\n",
    "#turn spark df object into delta table and save it in gold-container in your datalake\n",
    "spark_df.write.format(\"delta\").save(\"abfss://'your_container_name'@'your_data_lake_name'.dfs.core.windows.net/'your_deltatable_name'\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Call Center Analytics - PoC",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
